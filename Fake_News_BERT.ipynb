{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1FYhWUKX-zM"
   },
   "source": [
    "# Loading and Displaying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIKoEd4kGZvF",
    "outputId": "2ec49b2c-5cc1-43db-d866-d1cfbe737899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   title  \\\n",
      "0      As U.S. budget fight looms, Republicans flip t...   \n",
      "1      U.S. military to accept transgender recruits o...   \n",
      "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3      FBI Russia probe helped by Australian diplomat...   \n",
      "4      Trump wants Postal Service to charge 'much mor...   \n",
      "...                                                  ...   \n",
      "21412  'Fully committed' NATO backs new U.S. approach...   \n",
      "21413  LexisNexis withdrew two products from Chinese ...   \n",
      "21414  Minsk cultural hub becomes haven from authorities   \n",
      "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
      "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
      "\n",
      "                                                    text       subject  \\\n",
      "0      WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1      WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2      WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3      WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4      SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "...                                                  ...           ...   \n",
      "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...     worldnews   \n",
      "21413  LONDON (Reuters) - LexisNexis, a provider of l...     worldnews   \n",
      "21414  MINSK (Reuters) - In the shadow of disused Sov...     worldnews   \n",
      "21415  MOSCOW (Reuters) - Vatican Secretary of State ...     worldnews   \n",
      "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...     worldnews   \n",
      "\n",
      "                     date  \n",
      "0      December 31, 2017   \n",
      "1      December 29, 2017   \n",
      "2      December 31, 2017   \n",
      "3      December 30, 2017   \n",
      "4      December 29, 2017   \n",
      "...                   ...  \n",
      "21412    August 22, 2017   \n",
      "21413    August 22, 2017   \n",
      "21414    August 22, 2017   \n",
      "21415    August 22, 2017   \n",
      "21416    August 22, 2017   \n",
      "\n",
      "[21417 rows x 4 columns]\n",
      "Length (rows): 21417\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "real_news = pd.read_csv(r\"C:\\Users\\pothi\\Downloads\\archive\\News _dataset\\True.csv\")\n",
    "\n",
    "# Printing head and tail\n",
    "print(real_news)\n",
    "\n",
    "# Print length => rows\n",
    "print(\"Length (rows):\", len(real_news))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTBA9Fg9GiPq",
    "outputId": "e21f6e62-c4e6-415b-be19-562cd86e8db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   title  \\\n",
      "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
      "...                                                  ...   \n",
      "23476  McPain: John McCain Furious That Iran Treated ...   \n",
      "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
      "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
      "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
      "23480  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
      "\n",
      "                                                    text      subject  \\\n",
      "0      Donald Trump just couldn t wish all Americans ...         News   \n",
      "1      House Intelligence Committee Chairman Devin Nu...         News   \n",
      "2      On Friday, it was revealed that former Milwauk...         News   \n",
      "3      On Christmas day, Donald Trump announced that ...         News   \n",
      "4      Pope Francis used his annual Christmas Day mes...         News   \n",
      "...                                                  ...          ...   \n",
      "23476  21st Century Wire says As 21WIRE reported earl...  Middle-east   \n",
      "23477  21st Century Wire says It s a familiar theme. ...  Middle-east   \n",
      "23478  Patrick Henningsen  21st Century WireRemember ...  Middle-east   \n",
      "23479  21st Century Wire says Al Jazeera America will...  Middle-east   \n",
      "23480  21st Century Wire says As 21WIRE predicted in ...  Middle-east   \n",
      "\n",
      "                    date  \n",
      "0      December 31, 2017  \n",
      "1      December 31, 2017  \n",
      "2      December 30, 2017  \n",
      "3      December 29, 2017  \n",
      "4      December 25, 2017  \n",
      "...                  ...  \n",
      "23476   January 16, 2016  \n",
      "23477   January 16, 2016  \n",
      "23478   January 15, 2016  \n",
      "23479   January 14, 2016  \n",
      "23480   January 12, 2016  \n",
      "\n",
      "[23481 rows x 4 columns]\n",
      "Length (rows): 23481\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "fake_news = pd.read_csv(r\"C:\\Users\\pothi\\Downloads\\archive\\News _dataset\\Fake.csv\")\n",
    "\n",
    "# Printing head and tail\n",
    "print(fake_news)\n",
    "\n",
    "# Print length => rows\n",
    "print(\"Length (rows):\", len(fake_news))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvEKkMnwYCoO"
   },
   "source": [
    "# Combining and Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "rADmIBu0Gj-g",
    "outputId": "d8ee8aff-75aa-4185-d136-81f943ca5273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully and combined CSV generated!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>UK's May 'receiving regular updates' on London...</td>\n",
       "      <td>LONDON (Reuters) - British Prime Minister Ther...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>TIFFANY &amp; CO. TAKES BIG RISK…Sides AGAINST Tru...</td>\n",
       "      <td>Who knew? Tiffany &amp; Co. just decided to show i...</td>\n",
       "      <td>politics</td>\n",
       "      <td>May 9, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>U.S. 'very concerned' by violence around Iraq'...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. State Departme...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 16, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Democrat Takes Trump To The Woodshed In Fiery...</td>\n",
       "      <td>Donald Trump is getting absolutely hammered fo...</td>\n",
       "      <td>News</td>\n",
       "      <td>September 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>Trump declares national emergency on opioid abuse</td>\n",
       "      <td>BEDMINSTER, N.J. (Reuters) - U.S. President Do...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>August 10, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19215</th>\n",
       "      <td>PRICELESS! MILO DESTROYS Heckling Muslim Woman...</td>\n",
       "      <td></td>\n",
       "      <td>left-news</td>\n",
       "      <td>Feb 1, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9628</th>\n",
       "      <td>White House: Puerto Rico debt crisis could ham...</td>\n",
       "      <td>WASHINGTON (Reuters) - The White House said on...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>May 9, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>ERIC TRUMP REMOVES ANY DOUBT About Internation...</td>\n",
       "      <td>Eric Trump, in his first interview since his f...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Mar 20, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4652</th>\n",
       "      <td>House Democratic leader Pelosi says intel pane...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. House of Represent...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>March 28, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13503</th>\n",
       "      <td>WHOA! 8 ACTUAL QUOTES FROM HILLARY That Prove ...</td>\n",
       "      <td>Hillary came out with a heavily edited TV ad y...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Jul 16, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "19999  UK's May 'receiving regular updates' on London...   \n",
       "10934  TIFFANY & CO. TAKES BIG RISK…Sides AGAINST Tru...   \n",
       "17289  U.S. 'very concerned' by violence around Iraq'...   \n",
       "312     Democrat Takes Trump To The Woodshed In Fiery...   \n",
       "2241   Trump declares national emergency on opioid abuse   \n",
       "...                                                  ...   \n",
       "19215  PRICELESS! MILO DESTROYS Heckling Muslim Woman...   \n",
       "9628   White House: Puerto Rico debt crisis could ham...   \n",
       "11355  ERIC TRUMP REMOVES ANY DOUBT About Internation...   \n",
       "4652   House Democratic leader Pelosi says intel pane...   \n",
       "13503  WHOA! 8 ACTUAL QUOTES FROM HILLARY That Prove ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "19999  LONDON (Reuters) - British Prime Minister Ther...     worldnews   \n",
       "10934  Who knew? Tiffany & Co. just decided to show i...      politics   \n",
       "17289  WASHINGTON (Reuters) - The U.S. State Departme...     worldnews   \n",
       "312    Donald Trump is getting absolutely hammered fo...          News   \n",
       "2241   BEDMINSTER, N.J. (Reuters) - U.S. President Do...  politicsNews   \n",
       "...                                                  ...           ...   \n",
       "19215                                                        left-news   \n",
       "9628   WASHINGTON (Reuters) - The White House said on...  politicsNews   \n",
       "11355  Eric Trump, in his first interview since his f...      politics   \n",
       "4652   WASHINGTON (Reuters) - U.S. House of Represent...  politicsNews   \n",
       "13503  Hillary came out with a heavily edited TV ad y...      politics   \n",
       "\n",
       "                      date  label  \n",
       "19999  September 15, 2017       1  \n",
       "10934          May 9, 2017      0  \n",
       "17289    October 16, 2017       1  \n",
       "312     September 25, 2017      0  \n",
       "2241      August 10, 2017       1  \n",
       "...                    ...    ...  \n",
       "19215          Feb 1, 2017      0  \n",
       "9628          May 9, 2016       1  \n",
       "11355         Mar 20, 2017      0  \n",
       "4652       March 28, 2017       1  \n",
       "13503         Jul 16, 2016      0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "# Combine the datasets and shuffle\n",
    "fake_news['label'] = 0    # adds a column\n",
    "real_news['label'] = 1\n",
    "\n",
    "data = pd.concat([fake_news, real_news]).sample(frac=1.0)   # frac 1.0 => returns all the data (rows)\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "data.to_csv('Combined.csv', index=False)\n",
    "\n",
    "print(\"Data loaded successfully and combined CSV generated!\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkv-BogQYK1S"
   },
   "source": [
    "# Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K_hH1GM2GkAK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace characters that are not between a to z or A to Z with whitespace\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert all characters into lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove inflectional morphemes like \"ed\", \"est\", \"s\", and \"ing\" from their token stem\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ISfFCMaX59L"
   },
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loZ3t2bONI9d",
    "outputId": "561d3879-4b29-4c14-beba-5eafd812611d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1764\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     31\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, TFBertForSequenceClassification\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check if GPU is available\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mis_gpu_available():\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1755\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1754\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1752\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1768\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU:0'\n",
    "print('Using device:', device_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Combined.csv')\n",
    "\n",
    "# Preprocess the text using the preprocessing function\n",
    "print(\"\\nPreprocessing data...\")\n",
    "\n",
    "data['title_preprocessed'] = data['title'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_ratio = 0.64\n",
    "val_ratio = 0.16\n",
    "test_ratio = 0.2\n",
    "\n",
    "print(\"\\nSplitting Data...\")\n",
    "\n",
    "train_data = data.sample(frac=train_ratio, random_state=42)\n",
    "remaining_data = data.drop(train_data.index)\n",
    "\n",
    "val_data = remaining_data.sample(frac=val_ratio/(val_ratio+test_ratio), random_state=42)\n",
    "test_data = remaining_data.drop(val_data.index)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the titles and convert them into BERT input tensors\n",
    "print(\"\\nTokenizing and Converting...\")\n",
    "\n",
    "train_inputs = tokenizer(list(train_data['title_preprocessed']), truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "val_inputs = tokenizer(list(val_data['title_preprocessed']), truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "test_inputs = tokenizer(list(test_data['title_preprocessed']), truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "\n",
    "# Convert the labels into TensorFlow tensors\n",
    "train_labels = tf.convert_to_tensor(list(train_data['label']))\n",
    "val_labels = tf.convert_to_tensor(list(val_data['label']))\n",
    "test_labels = tf.convert_to_tensor(list(test_data['label']))\n",
    "\n",
    "# Extract token tensors, segment tensors, and mask tensors from the BERT inputs\n",
    "train_token_tensors = train_inputs['input_ids']\n",
    "train_segment_tensors = train_inputs['token_type_ids']\n",
    "train_mask_tensors = train_inputs['attention_mask']\n",
    "\n",
    "val_token_tensors = val_inputs['input_ids']\n",
    "val_segment_tensors = val_inputs['token_type_ids']\n",
    "val_mask_tensors = val_inputs['attention_mask']\n",
    "\n",
    "test_token_tensors = test_inputs['input_ids']\n",
    "test_segment_tensors = test_inputs['token_type_ids']\n",
    "test_mask_tensors = test_inputs['attention_mask']\n",
    "\n",
    "# Build the BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "print(\"\\nCompiling Model...\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the BERT model\n",
    "print(\"\\nTraining Model...\")\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 4\n",
    "\n",
    "history = model.fit([train_token_tensors, train_segment_tensors, train_mask_tensors], train_labels, batch_size=batch_size, epochs=num_epochs, validation_data=([val_token_tensors, val_segment_tensors, val_mask_tensors], val_labels))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"\\nSaving Model...\")\n",
    "\n",
    "model.save_pretrained('/content/bertv3_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.12.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.16.1 requires keras>=3.0.0, but you have keras 2.12.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.7 MB 69.0 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.1/1.7 MB 156.1 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.7 MB 156.1 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.7 MB 145.6 kB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.1/1.7 MB 145.6 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.1/1.7 MB 204.8 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.1/1.7 MB 200.1 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.2/1.7 MB 229.4 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.2/1.7 MB 229.4 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.2/1.7 MB 229.4 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.2/1.7 MB 229.4 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.2/1.7 MB 229.4 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 222.6 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 222.6 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 214.5 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 214.5 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 0.2/1.7 MB 218.3 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.3/1.7 MB 264.1 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.7 MB 280.8 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.7 MB 280.8 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.7 MB 280.8 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.7 MB 280.8 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.7 MB 280.8 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.7 MB 246.7 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.4/1.7 MB 259.0 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.4/1.7 MB 259.0 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.4/1.7 MB 259.0 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.7 MB 264.9 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.7 MB 264.9 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.7 MB 264.9 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.7 MB 254.4 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 0.4/1.7 MB 262.1 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 260.9 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 260.9 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 263.3 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 263.3 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 263.3 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.7 MB 266.4 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.6/1.7 MB 282.4 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.6/1.7 MB 301.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.6/1.7 MB 301.6 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.6/1.7 MB 290.7 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.6/1.7 MB 291.8 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.7/1.7 MB 311.2 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.7/1.7 MB 311.2 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 300.6 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 300.6 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 300.6 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 300.6 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 286.2 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 286.2 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 286.2 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 288.7 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 0.8/1.7 MB 295.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.9/1.7 MB 325.7 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.9/1.7 MB 325.7 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 0.9/1.7 MB 333.2 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 0.9/1.7 MB 333.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.0/1.7 MB 332.9 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 338.0 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 338.0 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 334.4 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.1/1.7 MB 352.3 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.1/1.7 MB 357.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.2/1.7 MB 361.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.2/1.7 MB 365.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.2/1.7 MB 370.3 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 380.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 380.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 380.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 380.2 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.3/1.7 MB 376.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.3/1.7 MB 376.6 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.4/1.7 MB 380.6 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.4/1.7 MB 380.6 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.4/1.7 MB 380.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 371.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 371.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.7 MB 384.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 383.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 385.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 389.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 395.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.7 MB 405.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.7 MB 401.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 409.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 409.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 407.9 kB/s eta 0:00:00\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.12.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUlYf6JWXyKQ"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX3mw-GCNpuY",
    "outputId": "e248cd1a-dab4-4d3f-a7df-dde239bc2c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 26s 186ms/step - loss: 0.1080 - accuracy: 0.9700\n",
      "Test loss: 10.803%\n",
      "Test accuracy: 97.004%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate([test_token_tensors, test_segment_tensors, test_mask_tensors], test_labels, batch_size=batch_size)\n",
    "print(f'Test loss: {loss * 100:.3f}%')\n",
    "print(f'Test accuracy: {accuracy * 100:.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CpwrDHrX1FF"
   },
   "source": [
    "# Using the Model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9l9GG-r5VHZ",
    "outputId": "9ec2c470-7f07-4f0e-b0d9-cbec5063f73c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /content/bertv3_model were not used when initializing TFBertForSequenceClassification: ['dropout_149']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/bertv3_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Enter News Title: USA is bankrupted\n",
      "Preprocessed Text: usa is bankrupt\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "\n",
      "Fake news\n",
      "\n",
      "Probability of being fake: 76.43%\n",
      "Probability of being real: 23.57%\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = TFBertForSequenceClassification.from_pretrained('/content/bertv3_model')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input text\n",
    "input_text = input(\"\\n\\nEnter News Title: \")\n",
    "\n",
    "# Preprocess the input text\n",
    "preprocessed_text = preprocess_text(input_text)\n",
    "\n",
    "print(\"Preprocessed Text:\", preprocessed_text)\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "inputs = tokenizer(preprocessed_text, truncation=True, padding='max_length', max_length=42, return_tensors='tf')\n",
    "\n",
    "# Extract input tensors\n",
    "token_tensors = inputs['input_ids']\n",
    "segment_tensors = inputs['token_type_ids']\n",
    "mask_tensors = inputs['attention_mask']\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([token_tensors, segment_tensors, mask_tensors])\n",
    "logits = predictions.logits[0]\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "predicted_label = tf.argmax(probabilities)\n",
    "\n",
    "# Print the predicted label and probabilities\n",
    "if predicted_label == 0:\n",
    "    print(\"\\n*-*-Fake News-*-*\")\n",
    "else:\n",
    "    print(\"\\n*-*-Real News-*-*\")\n",
    "\n",
    "print(\"\\nProbability of being fake: {:.2%}\".format(probabilities[0]))\n",
    "print(\"Probability of being real: {:.2%}\".format(probabilities[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ewn1IRYX7UA"
   },
   "source": [
    "BREAKING: MUSLIM OPENS FIRE ON JOURNALISTS [Video]\n",
    "\n",
    " WATCH: Wolf Blitzer Makes Republican Throw Temper Tantrum Over Trumpâ€™s Nazi Problem\n",
    "\n",
    " Boston men jailed for Trump-inspired hate crime attack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2p0PCN6b-90A",
    "outputId": "84086805-f856-4033-923c-04d22897ddaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
